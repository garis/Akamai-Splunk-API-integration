{"alert_action_builder": {"modular_alerts": [{"description": "Disable and enable Akamai inputs", "largeIcon": "iVBORw0KGgoAAAANSUhEUgAAAEgAAABICAYAAABV7bNHAAAFbElEQVR4nO2bX2gcRRzHf/cvd7mcCU3aeNfYxDRJ2yRe0+TyII02MWDFGh9EIRjTisYoInqiItZqSltKRWNPY0hzqX0TUSwFC6IRLEJ9KIotRpBqxVDoi4rkpSK0pD8flr2b2Z3dndnbu5k094Mf7OzMzv5+n3znz81dAMqmnF0AAHTpX0iIt+j2OLgH4uQr2ooFheVLJcrJEyslmBWlKtlglAV1AiQAwGyKp936YibOY1LUgdlUzjmfkWLShg8JKF4dVA5Sn5ukvHISjqCKSgLpstcJi3i8OsQEdGJPkxKQpMIBC/W4UFFRIPXLhvPxWDMF5PjuJqp8faZHGqRh2XCAUM/VqW3U/XDQ51ZFnkFSBg5mU/jzRIdtfakhSYdjBMCCUCCgj24qOE6ASqki6XCiFT5HQC/fe6upfntLVdEhSYcDFuoxAvr6xTYvln0EgAd44YzIBgMA+M7DDVyAljJdzDann2spmoqkwwEb9RgBXZvp8WrziAAwvyIA/Tm5lRvQ8jFrQNfEN4+OKpIOBxzUYwQk0pbT094A+vAP2ksEB7Mp/G+6GwEAa6MBrvYu4mDavDCYyXN0Xd1tCPfsRujc4QpOY20FV8KiHg76PAFk/1DfIzScSEy7f/ewWU1GD/AdavEm3N8Wyz1zY9Z6DipARQfEAI29SyccCrOHmZ3vPWkb1Cs7zRs+3mR5ntlzZ21BKkpZNly/iU70ljpNEUYAH/yEEGMEEalCOPyN1ubwmYLV4xaQCxVxqoeE8N6P5nuzF/lf+tTR/NAk/OLBTteAljmHGGZT+P3eLR4DmvnFvFJlf8uXu3dq9xo22w+v27d6Mvd44QKAFp0BsZZy/bqlJ18ORdjPBysQphe0NnOXpMPBbArnRhtdqchceccOGk60BmE8o12/f150PCO09iLM/c6s2z+UKJkLxm0DKPODtXpE4ZBeaZ6DFHYAsDqQtxte8gNXGFA4ytW5cez3GQ6xXC69nnp7IuK0urkAxPlyI6B998dzdeRHCpmAOFY3QUAWK5HRWdt/8tuIk89sVAIQAOCvBzvt6m0AvXnaDCjR6vjCylD+HNn4adv4l/tkvJkqX3kriQCAF95ot9y/6OV1sXzflw5pSeqbxr8m8/uu717dTPWzfKzH1FcoYPlh1gZQtJoGtI5v/2BMyi7JUMCH/051I2ZTeGNWC3xhoiNX/+XzrdTzsbBfaENIwiH70o9LOFScM3YDF2c++kuHkjWOgJINlaa6SNCHkaAPAz667cJEB86NNlHtyR8zGPsm+wr66bqlTBe2xyMeABo9JASIhJEerMf0YL0JUGciQt3DbAqrwn5mH6Tf1RrLXZ97TVt1ph/dYAvousV59XDvGjxPDGP3gARUFK8Ocsn+8pGk6b4+h5x5aRPVtqbST5WtYMyn2xAAcGakMXfvyEMNVHvjd/j69Vjf2gIBGSE9PcWtnvRgPTXhku2Ge9fgvl1xZt0/R7sQAHBiKMEFSP+VmV4mvytbymh9Pbm9jtmXzQS9yA8oVktDGniMql8by6vnvo5qqs4Kgs8wx5x6diNendpGgSZ/4rKFMWdYldvqw6a+Ph1vZsbBox4AgAFHSP4ADen1U45z0gp3k/E9eOAr9tn0zecuAemeHKBBPfiC7IS89LdZgM4qEJgqbmmyA1PFy4BsfL8doDIkDpMdoEz/lgfQEwoEqqx6dJMdqAwfEQG0GiEJm+yAlYazmiB9Xgig1QCpYFPin1lUhaNbvwLJKAtHtysKJKUsHN3+ViA5ZeHotkticsrDIU12skrD0U120krD0e0zkA/AyjcUMW9hkw1DKdXYWRkMp5XBcFoaylCEbBHcAzkrId6y2dn/XVgvlx45zVkAAAAASUVORK5CYII=", "smallIcon": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAAB2UlEQVRYhWNgoBz8R8N0BfJYHEAI0wSQ6ghsmHnAHfN/pjFVQyycUsfgcBBZjjpHiWOQHUQNR/lQwzHPuvTgbBVRdoocRZFjIkwFUUKFQCgRdBTFuQndATD+72lGJDuKao75P9P4f4mrOLFpiT4OetyhiyFGiqOwZ3E+0f8Mc+79Z7ANI8kxyJaT6yBUSQkliEN88yD8OfcQmAQHdQfL4HQoIUchBD3SIBZz8qA6pOPQf4bekxB2236iQoeBgeH/18mGFDoI2RGwEHFPxRlCjuq8WB30f6bxfx0pDpxy+ByE6RgYG5smZhYUOVwWEsKkOUhQ8j+DpDLFuY4MfAm7g9TM8SZcZD6ljsBiBo4oQ8Or05Vo4iBs0YbgzLqN00HI5QojA8P/wyVqGOUNOm2hyP3//0zj/7YqPFjl/880/j8hTAbZnocMGJZjCaXnXXooCXJiuCzeAlBOkA2nAwgkbjhACHLxQxzkFIc1arA5BJuFCxIU/q/LUMYp76jGS6SDkENq+nVa5iis6Qe3g2A4om5AHJRK55DA6xjCoTQCHcSCzUED6Si8YFA5hoGBgSF4MDkGBq4OJsfAAEVdaWo7BhkMKsdQ01E4szYlQJ8Mh9AV8GNxAD8lBgIAYiMK9tLX3UEAAAAASUVORK5CYII=", "label": "Akamai input status check", "short_name": "akamai_input_status_check", "parameters": [{"format_type": "text", "required": true, "name": "username", "label": "Username", "default_value": "-", "help_string": "Username to use for accessing REST API to disable and enable one input", "type": "", "value": "-"}, {"format_type": "text", "required": true, "name": "base_url", "label": "Base URL", "default_value": "https://localhost:8089", "help_string": "URL form the REST API call.", "type": "", "value": "https://localhost:8089"}, {"format_type": "dropdownlist", "required": true, "name": "input_type", "label": "Input type", "default_value": "akamai_siem", "help_string": "", "possible_values": {"akamai_siem": "akamai_siem", "conf_domains": "conf_domains", "prolexic_events": "prolexic_events", "prolexic_metrics": "prolexic_metrics"}, "type": "", "value": "akamai_siem"}, {"format_type": "text", "required": true, "name": "input_name", "label": "Input name", "default_value": "-", "help_string": "Name of the input", "type": "", "value": "-"}], "code": "\n# encoding = utf-8\n\nimport sys\nimport urllib\nimport requests\nfrom xml.dom import minidom\n\ndef process_event(helper, *args, **kwargs):\n    \"\"\"\n    # IMPORTANT\n    # Do not remove the anchor macro:start and macro:end lines.\n    # These lines are used to generate sample code. If they are\n    # removed, the sample code will not be updated when configurations\n    # are updated.\n\n    [sample_code_macro:start]\n\n    # The following example sends rest requests to some endpoint\n    # response is a response object in python requests library\n    response = helper.send_http_request(\"http://www.splunk.com\", \"GET\", parameters=None,\n                                        payload=None, headers=None, cookies=None, verify=True, cert=None, timeout=None, use_proxy=True)\n    # get the response headers\n    r_headers = response.headers\n    # get the response body as text\n    r_text = response.text\n    # get response body as json. If the body text is not a json string, raise a ValueError\n    r_json = response.json()\n    # get response cookies\n    r_cookies = response.cookies\n    # get redirect history\n    historical_responses = response.history\n    # get response status code\n    r_status = response.status_code\n    # check the response status, if the status is not sucessful, raise requests.HTTPError\n    response.raise_for_status()\n\n\n    # The following example gets and sets the log level\n    helper.set_log_level(helper.log_level)\n\n    # The following example gets account information\n    user_account = helper.get_user_credential(\"<account_name>\")\n\n    # The following example gets the setup parameters and prints them to the log\n    sslcertverification = helper.get_global_setting(\"sslcertverification\")\n    helper.log_info(\"sslcertverification={}\".format(sslcertverification))\n\n    # The following example gets the alert action parameters and prints them to the log\n    username = helper.get_param(\"username\")\n    helper.log_info(\"username={}\".format(username))\n\n    base_url = helper.get_param(\"base_url\")\n    helper.log_info(\"base_url={}\".format(base_url))\n\n    input_type = helper.get_param(\"input_type\")\n    helper.log_info(\"input_type={}\".format(input_type))\n\n    input_name = helper.get_param(\"input_name\")\n    helper.log_info(\"input_name={}\".format(input_name))\n\n\n    # The following example adds two sample events (\"hello\", \"world\")\n    # and writes them to Splunk\n    # NOTE: Call helper.writeevents() only once after all events\n    # have been added\n    helper.addevent(\"hello\", sourcetype=\"sample_sourcetype\")\n    helper.addevent(\"world\", sourcetype=\"sample_sourcetype\")\n    helper.writeevents(index=\"summary\", host=\"localhost\", source=\"localhost\")\n\n    # The following example gets the events that trigger the alert\n    events = helper.get_events()\n    for event in events:\n        helper.log_info(\"event={}\".format(event))\n\n    # helper.settings is a dict that includes environment configuration\n    # Example usage: helper.settings[\"server_uri\"]\n    helper.log_info(\"server_uri={}\".format(helper.settings[\"server_uri\"]))\n    [sample_code_macro:end]\n    \"\"\"\n\n    helper.log_info(\"Alert action reload_input started.\")\n\n    # TODO: Implement your alert action logic here\n\n    user_account = helper.get_user_credential(str(helper.get_param(\"username\")))\n    \n    clientid=user_account[\"username\"]\n    secret=user_account[\"password\"]\n    \n    #helper.log_info(\"user_account={}\".format(user_account))\n\n    input_type = str(helper.get_param(\"input_type\"))    \n    input_name = str(helper.get_param(\"input_name\"))\n    helper.log_info(\"input_name={}\".format(input_name))\n    \n    base_url = str(helper.get_param(\"base_url\"))\n\n    helper.log_info(\"base_url={}\".format(base_url))\n    \n    # something like -> https://localhost:8089/servicesNS/nobody/akamai-api-integration/data/inputs/akamai_siem/Test\n    url = base_url + \"/servicesNS/\" + clientid +\"/akamai-api-integration/data/inputs/\"+ input_type + \"/\" + input_name\n    \n    payload={}\n    \n    helper.log_info(\"AKAMAI API INTEGRATION ACTION: Username: \"+ clientid + \" Input: \" + input_name + \" URL: \" + url )\n    \n    def splunk_auth():\n        data = {'username': clientid, 'password': secret}\n        auth_url = base_url + \"/services/auth/login\"\n        \n        #helper.log_info(\"data={}\".format(data))\n        helper.log_info(\"auth_url={}\".format(auth_url))\n        \n        session_response = \"\"\n        session_response_text = \"\"\n        \n        try:\n            servercontent = requests.post(base_url+'/services/auth/login', data=data, verify=False)  \n            session_response = str(servercontent.status_code)\n            session_response_text = str(servercontent.text)\n            helper.log_info(\"servercontent.text={}\".format(servercontent.text))\n        except:\n            e = sys.exc_info()[0]\n            helper.log_error('AKAMAI API INTEGRATION ACTION: Unable to authenticate to Splunk API.')\n            helper.log_info('AKAMAI API INTEGRATION ACTION: Exception accessing Splunk API: ' + str(e))\n            helper.log_info('AKAMAI API INTEGRATION ACTION: Exception accessing Splunk API: ' + session_response_text)\n        \n        if session_response.startswith('20'):       \n            sessionkey = minidom.parseString(servercontent.text).getElementsByTagName('sessionKey')[0].childNodes[0].nodeValue\n            helper.log_info('AKAMAI API INTEGRATION ACTION: Successfully acquired Splunk authentication token.')\n            print('got session key')\n        else:\n            helper.log_error('AKAMAI API INTEGRATION ACTION: Unable to acquire Splunk authentication token.')\n            helper.log_error('AKAMAI API INTEGRATION ACTION: '+session_response_text)\n            print('could not pull session key')\n            sys.exit()\n        \n        return sessionkey\n\n    def disable_input(session_key):\n        disable_url = url + '/disable'\n        helper.log_info('AKAMAI API INTEGRATION ACTION: Sending disable request: ' + str(disable_url))\n\n        disable_response = \"\"\n        try:    \n            headers={'Authorization': 'Splunk %s' % session_key}\n            disable_response = requests.request(\"POST\", disable_url, headers=headers, verify=False).text\n        \n        except:\n            e = sys.exc_info()[0]\n            helper.log_error('AKAMAI API INTEGRATION ACTION: Unable to disable input.')\n            helper.log_debug('AKAMAI API INTEGRATION ACTION: Error response was: ' + str(e))\n        \n        helper.log_info('AKAMAI API INTEGRATION ACTION: Disable request response: ' + str(disable_response))\n\n    \n    def enable_input(session_key):\n        headers={'Authorization': 'Splunk %s' % session_key}\n        enable_url = url + '/enable' \n        helper.log_info('AKAMAI API INTEGRATION ACTION: Sending enable request: ' + str(enable_url))\n        \n        enable_response = \"\"\n\n        try:\n            enable_response = requests.request(\"POST\", enable_url, headers=headers, verify=False).text\n            \n        except:\n            e = sys.exc_info()[0]\n            helper.log_error('AKAMAI API INTEGRATION ACTION: Unable to enable input.')\n            helper.log_debug('AKAMAI API INTEGRATION ACTION: Error response was: ' + str(e))\n            \n        helper.log_info('AKAMAI API INTEGRATION ACTION: Enable request response: ' + str(enable_response))\n\n    session_key=splunk_auth()\n    disable_input(session_key)\n    enable_input(session_key)\n\n    return 0\n", "uuid": "25a98672438e445094bc95b04f110e22"}]}, "basic_builder": {"appname": "akamai-api-integration", "friendly_name": "Akamai API integration", "version": "1.2.6", "author": "Riccardo Gasparini", "description": "Akamai API add on for Splunk", "theme": "#009CDE", "large_icon": "iVBORw0KGgoAAAANSUhEUgAAAEgAAABICAYAAABV7bNHAAAE4UlEQVR4nO2cXWgcVRTHz35lN8lmYzdN2BiNpt20zca1JhsUjNaaB8USQUEI/ciLMS0KNSIo1mqgllKRttE0xGz0SUQs1CIBPypYEBSDoQlNi2hbBKFPQqlSbUBJjw/T3Z27c+fundmZOTft/uEPO3Pv3Dn/X87cCQkJQEXK6QcAWAYAtOjLADBEUK/r6gfrMGR8DQDiHuZwXP+CO2B4XvAoU9mqA++g8HzJ/Yj2RQmm2LMuZ7WkT4AeiJnrXMwtJWoAMj7vWnqB2mwWS2nPtM/FECse0o+E4ZxyyHEqN3QzwHEN0mEFQjltx3S/AmGUhkQdQmlI1MV74ZN24cwrULxX9tsBRF20167AKeHdsnB6FChW6S6iLpLSMxVAZXYRdXEqeIszgD78jTV9MNe76DPLYA7NsmMNdyA8OojQuYk6oCuAxBf1PsPCiUS18w8PGLup2IEgdWCr/sgaoKHDbOBQ2NhNpbznOHXosrpom+nE29exQesatI4oBnD0DEI0brw+Uotw4FttzoFT1KFtA/rTdKIewrunjeemfpG/6XNHCo+m+mbEnzT5s/FNlT1fOO56TDvXsl78eN19L3VYO/6yNCDeqzz3eW134TgU4V8frEKYWNTmTF+gDmzV18SA7tnEwqmpRxge0z6/N2/9hskehOmL1KGtGgAAbuMOjs2Zd085N61eMXtQHlCKOyh6vOgL9xTQI5YAhWukFsdshnHv2lruOCWAjuYIYjaDP+3Z4CAgyZsXA9r7RCI/1hqvUgKQvj5nAEm+ia5PdRsAnR1N5ceP71qjBCAAwF/f6hSNCwC9OWME1JwsecPqkC8fPl4T4H6VcsefDrcxx5feTiMA4MIbHQbAxdc2RgtrX9ivhVy+8YX541Dh+67vX13PrLP8frdhrVDAJwTUyR2sibGAGlttta0oZCjgw3/GuxCzGbw+pRW+OJrKj3+1O8lcHw37DeDMXAxHv9bSRBdTR6kO4r/mzR4zSUD96fqSgNIt1YaxSNCHkaAPAz527uJoCqd33MXMT8RCpmvr1wr62bErYxuxIxGRBgSmE3bstwRID2OkrwlH+poMgDqbI8w5zGawNuznrqH3Q8lo/vPsa9pbZ2LrnUJA/00a90LMZnCgZxXO6x5j+4AsdFEiFpRq+98Ppg3nc3vIqZfXMXPrq/3MsRmMkyPtCAA4ua01f+7g0y3M/HDQx11rqHe1WaYlOUDFkHaOS3fPSF8Ts+Hq5w30rMK9WxLcsctHNiIA4Gh/sxSgRCzIHH/zUjvzOAEAPvtgA3ctwQbN/Er6LyGgaJyFtHk7M746Wuiex1MxZswMgq9ojznx/Br8e/w+BvQHg4U9ZwNnzzA7bm8KG9Y6NtwmhC16vAAABkt2kT/AQnr9RMk9aYXbILkL933N/9n0zWebgHJOb2ZBPfkidSAn/TEP0OcKFKaKTUVdmCquABL4KRGgCiQJURdI6S9kAD2gQKHKdk9O1IVS+BUrgG5FSJZ1RoGivXLADiBQoHAv/J1dOLcKpLJV+WMWCR1VIIyycHKaUyCUU65ymE1eCwqEUxZOTu8oENKuPVPSxRArHo5e1KFlfNG19JKaAXoIZq53MbdlUcPQe87lrLZV+fc4FuQlGPJ9phxtBXegLAFAo4c5PNM5sPdP3q4CwAsE9VYk0v/RONRTc1P6+gAAAABJRU5ErkJggg==", "small_icon": "iVBORw0KGgoAAAANSUhEUgAAACQAAAAkCAYAAADhAJiYAAABsUlEQVRYhc3YP0vDQBQA8CfVihqVDoL1f1vpIrjcoEsHXXRxjbpIQfy3ODk7iIoKBZ3kpKCbg+DupOjkJ3AQuvoB/ADPIZy9JC9tmuTu8uDRXC+59+PlAmkB4oUNAOhJK+aaHcU0AWiXGRWQ+wgQb44lhYkL8Wbk2FSAiYx6UYgR2RMWs60B01GndGFCoXRjWqJ+DYLI/WQKQ3bpjDxpaASh3kCo2NpB7snRogNZO3TG9UYzNaGaX67uOoX7LDfk4h2h9ukcn79qBMkI0ZGVHa0d8mPEMXVRplslLEODcnmEfEl1N6g8oUHlBfIC5AyRM9c4LoJYI+CWefJpr6gERKQ0uPsOBIniyBl2AeDHUdkFpD4XCwOInGFl1iLnkTO8tifkOqfgK0506edq/n8B5Axv1iddY7kAcoZTuWwgwJvUpnaj+ocd0PIWeWsoCFXwoTqDz/ulwPml8iC5f/wguVO3XyqfKHL/BINEbhwbAV1q7kRLTPsuGQKZfEHrpUAmu9QyUoUBADhIE0bEmwZMthMQQMp+SsuRKoyIxwQghaQwcsxFgIT+UyGJqBKA8TgL/gGe/m2w2gmUOwAAAABJRU5ErkJggg==", "visible": true, "tab_version": "4.1.0", "tab_build_no": "0", "build_no": 8}, "data_input_builder": {"datainputs": [{"index": "default", "sourcetype": "akamai:json_conf", "interval": "604800", "use_external_validation": true, "streaming_mode_xml": true, "name": "conf_domains", "title": "Akamai DNS and GTM", "description": "Akamai Edge DNS Zone and GTM Management API v2 add on for Splunk", "type": "customized", "parameters": [{"name": "api_base_url", "label": "API base url", "help_string": "https://akab-yyyyyyyyyyyyyyyy-xxxxxxxxxxxxxxxx.luna.akamaiapis.net/", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "a"}, {"name": "api_client_token", "label": "API client token", "help_string": "", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "a"}, {"name": "api_client_secret", "label": "API client secret", "help_string": "", "required": true, "format_type": "password", "default_value": "", "placeholder": "", "type": "password", "value": "a"}, {"name": "api_access_token", "label": "API access token", "help_string": "", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "a"}, {"name": "gtm_configuration", "label": "GTM configuration", "help_string": "Check for GTM config", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}, {"name": "dns_zones", "label": "DNS zones", "help_string": "Check for DNS zones", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}], "data_inputs_options": [{"type": "customized_var", "name": "api_base_url", "title": "API base url", "description": "https://akab-yyyyyyyyyyyyyyyy-xxxxxxxxxxxxxxxx.luna.akamaiapis.net/", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_client_token", "title": "API client token", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_client_secret", "title": "API client secret", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "password", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_access_token", "title": "API access token", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "gtm_configuration", "title": "GTM configuration", "description": "Check for GTM config", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "dns_zones", "title": "DNS zones", "description": "Check for DNS zones", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}], "code": "\n# encoding = utf-8\n\nimport os\nimport sys\nimport time\nimport datetime\nimport requests\nimport hashlib\nimport json\nimport re\nfrom time import sleep\nfrom akamai.edgegrid import EdgeGridAuth  # from https://github.com/akamai/AkamaiOPEN-edgegrid-python\n\n# Special thanks to A.M. (aka Pastea) for the help with all this code\n\n'''\n    IMPORTANT\n    Edit only the validate_input and collect_events functions.\n    Do not edit any other part in this file.\n    This file is generated only once when creating the modular input.\n'''\n\ndef validate_input(helper, definition):\n    \"\"\"Implement your own validation logic to validate the input stanza configurations\"\"\"\n    pass\n\ndef formatDomain(obj_dict):\n    if isinstance(obj_dict,dict):\n        keys = list(obj_dict.keys())\n        for k in keys:\n            if k in [\"links\"]:\n                obj_dict.pop(\"links\",None)\n                continue\n            if isinstance(obj_dict[k],dict):\n                formatDomain(obj_dict[k])\n            elif isinstance(obj_dict[k],list):\n                for e in obj_dict[k]:\n                    formatDomain(e)\n\n# a lot of parsing to trasform the zone dump of a domain in a JSON easy to work in Splunk.\n# Regex man was here :)\n# In regex man I trust\ndef formatMasterZoneFile(zone,file,helper):\n    lines = file.splitlines()\n    output = []\n    origin=\"%s.\" % zone\n    for idx, l in enumerate(lines):\n        l = l.strip()\n        helper.log_debug(l)\n        if l.startswith(\";;\"):\n            file_generated = re.match(\"^;; File Generated at (?P<DATETIME>.*?)$\",l)\n            if file_generated:\n                file_generated = file_generated.group(\"DATETIME\")\n            last_modified = re.match(\"^;; Last Modified at (?P<DATETIME>.*?)\\[.*?$\",l)\n            if last_modified:\n                last_modified = last_modified.group(\"DATETIME\")\n            custom = re.match(\"^;; Akamai (?P<CLASS>.*?) record for (?P<FQDN>.*?) points to (?P<TYPE>.*?) (?P<VALUE>.*?)$\",l)\n            if custom:\n                d = {\"LAST_MODIFIED\":\"%s\" % last_modified, \"ZONE\":\"%s\" % origin, \"NAME\":\"%s\" % custom[\"FQDN\"],\"CLASS\":\"%s\" % custom[\"CLASS\"],\"TYPE\":\"%s\" % custom[\"TYPE\"], \"VALUE\":\"%s\" % custom[\"VALUE\"], \"FQDN\":\"%s\" % custom[\"FQDN\"]}\n                output.append(d)\n        elif l.startswith(\"$\"):\n            origin = re.match(\"^\\$ORIGIN\\s+(?P<FQDN>.*?)$\",l)\n            if origin:\n                origin = origin.group(\"FQDN\")\n        else:\n            row = re.match(\"^(?P<NAME>.*?)\\s+(?P<TTL>.*?)\\s+(?P<CLASS>.*?)\\s+(?P<TYPE>.*?)\\s+(?P<VALUE>.*?)$\",l)\n            d = {\"LAST_MODIFIED\":\"%s\" % last_modified, \"ZONE\":\"%s\" % origin, \"NAME\":\"%s\" % row[\"NAME\"],\"TTL\":\"%s\" % row[\"TTL\"],\"CLASS\":\"%s\" % row[\"CLASS\"],\"TYPE\":\"%s\" % row[\"TYPE\"], \"VALUE\":\"%s\" % row[\"VALUE\"]}\n            if d[\"NAME\"][-1]!=\".\" and not \"._\" in d[\"NAME\"]:\n                d[\"FQDN\"]=\"%s.%s\" % (d[\"NAME\"],origin)\n            else:\n                d[\"FQDN\"]=d[\"NAME\"]\n            output.append(d)\n    return output\n    \ndef collect_events(helper, ew):\n    \n    helper.log_info(\"Input {} has started.\".format(str(helper.get_input_stanza_names())))\n    \n    # setup for the proxy variables if needed\n    proxy = helper.get_proxy()\n    proxies = None\n    if proxy:\n        proxies = \\\n                {'https': '{}://{}:{}@{}:{}/'.format(proxy['proxy_type'\n                 ], proxy['proxy_username'], proxy['proxy_password'],\n                 proxy['proxy_url'], proxy['proxy_port']),\n                 'http': '{}://{}:{}@{}:{}/'.format(proxy['proxy_type'\n                 ], proxy['proxy_username'], proxy['proxy_password'],\n                 proxy['proxy_url'], proxy['proxy_port'])}\n    \n    # get all the input variables configured\n    opt_api_base_url = helper.get_arg('api_base_url')\n    opt_api_client_token = helper.get_arg('api_client_token')\n    opt_api_client_secret = helper.get_arg('api_client_secret')\n    opt_api_access_token = helper.get_arg('api_access_token')\n    opt_gtm_configuration = helper.get_arg(\"gtm_configuration\")\n    opt_dns_zones = helper.get_arg(\"dns_zones\")\n    global_sslcertverification = helper.get_global_setting(\"sslcertverification\")\n    \n    # variable to save the current time and use it for the event time when logging\n    logTime = int(time.time())\n    \n    if opt_gtm_configuration == True:\n        \n        # run if the input is configured to collect the GTM config from the API\n        \n        akamaiAPIsession = requests.Session()\n        akamaiAPIsession.auth = EdgeGridAuth(opt_api_client_token,opt_api_client_secret, opt_api_access_token)\n        \n        # first get all the domains...\n        url = opt_api_base_url+\"/config-gtm/v1/domains\" \n        result = akamaiAPIsession.get(url,proxies=proxies,verify=global_sslcertverification)\n    \n        helper.log_debug('sent GTM GET request domains')\n\n        if result.status_code == 200:\n            \n            # ... and now that we have the list of all the domains\n            helper.log_debug('GET GTM response 200')\n            domains_list = result.json()\n\n            for d in domains_list[\"items\"]:\n    \n                # ... dump the GTM config for each one\n                helper.log_debug('logging '+d[\"name\"])\n                url = opt_api_base_url+\"/config-gtm/v1/domains/\"+d[\"name\"]\n                    \n                result = akamaiAPIsession.get(url,proxies=proxies,verify=global_sslcertverification)\n                helper.log_debug('sent GET request domain details')\n                if result.status_code == 200:\n\n                    helper.log_debug('GET response 200 domains details')\n                \n                    domain = result.json()\n                    \n                    # do a minimal parsing to have each record in a single JSON line\n                    formatDomain(domain)\n                    event = helper.new_event(time=logTime, source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=json.dumps(domain))\n                    # and log the data\n                    ew.write_event(event)\n                else:\n                    # dump the error in the logs\n                    helper.log_error(result.text)\n        else:\n            # dump the error in the logs\n            helper.log_error(result.text)\n            \n    if opt_dns_zones == True:\n        \n        # run if the input is configured to collect the DNS zones from the API\n        akamaiAPIsession = requests.Session()\n        akamaiAPIsession.auth = EdgeGridAuth(opt_api_client_token,\n            opt_api_client_secret, opt_api_access_token)\n    \n        helper.log_debug('sent GET DNS request domains')\n        url = opt_api_base_url+\"/config-dns/v2/zones?showAll=true\"\n        result = akamaiAPIsession.get(url,proxies=proxies, verify=global_sslcertverification)\n        zones_list = result.json()\n        if result.status_code == 200:\n            \n            # for each zone that we got from the previous request ...\n            helper.log_debug('GET DNS response 200')\n            for z in zones_list[\"zones\"]:\n                # log taht zone\n                event = helper.new_event(time=logTime, source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=json.dumps(z))\n                ew.write_event(event)\n        \n                # and try to dump the entire zone file\n                attempt=0\n                # since the DNS zone files are a lot of data sometimes there are errors.\n                # This will retry to get the zone with an exponential backoff if case of problems\n                while True:\n                    try:\n                        url = opt_api_base_url+\"/config-dns/v2/zones/\"+z[\"zone\"]+\"/zone-file\"\n                        result = akamaiAPIsession.get(url,proxies=proxies,headers={\"Accept\":\"text/dns\"},verify=global_sslcertverification)\n                        helper.log_debug(z[\"zone\"]+\" status: \"+str(result.status_code))\n                        if result.status_code in [200,404]:\n                            break\n                        else:\n                            raise Exception\n                    except Exception as e:\n                        # log the error\n                        helper.log_info(\"Exception \" + str(result.status_code))\n                        # and create a new session object\n                        akamaiAPIsession = requests.Session()\n                        akamaiAPIsession.auth = EdgeGridAuth(opt_api_client_token, opt_api_client_secret, opt_api_access_token)\n                        # it will work eventually\n                        attempt=attempt+1\n                        # or stall forever and ever\n                        sleep(attempt*1)\n                        pass\n\n                if result.status_code == 200:\n                    # now we have the zone dump\n                    helper.log_debug('GET DNS response 200')\n                    # perform a clean up and trasform in JSON\n                    record_set = formatMasterZoneFile(z[\"zone\"],result.text,helper)\n                    for r in record_set:\n                        #and log the data\n                        event = helper.new_event(time=logTime, source=helper.get_input_type(), index=helper.get_output_index(), sourcetype=helper.get_sourcetype(), data=json.dumps(r))\n                        ew.write_event(event)\n                else:\n                    # dump the error in the logs\n                    helper.log_error(result.text)    \n        else:\n            # dump the error in the logs\n            helper.log_error(result.text)\n    \n    helper.log_info(\"Input {} has ended.\".format(str(helper.get_input_stanza_names())))", "customized_options": [{"name": "api_base_url", "value": "a"}, {"name": "api_client_token", "value": "a"}, {"name": "api_access_token", "value": "a"}, {"name": "gtm_configuration", "value": true}, {"name": "dns_zones", "value": true}], "uuid": "6f78bcd0a3c44918a7f891bb60d32504", "sample_count": 0}, {"index": "default", "sourcetype": "akamai:json_event", "interval": "900", "use_external_validation": true, "streaming_mode_xml": true, "name": "prolexic_events", "title": "Akamai Prolexic events", "description": "Akamai Prolexic Analytics APIv2 events add on for Splunk", "type": "customized", "parameters": [{"name": "api_base_url", "label": "API base url", "help_string": "https://akab-yyyyyyyyyyyyyyyy-xxxxxxxxxxxxxxxx.luna.akamaiapis.net/", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "a"}, {"name": "api_client_token", "label": "API client token", "help_string": "", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "a"}, {"name": "api_client_secret", "label": "API client secret", "help_string": "", "required": true, "format_type": "password", "default_value": "", "placeholder": "", "type": "password", "value": "a"}, {"name": "api_access_token", "label": "API access token", "help_string": "", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "a"}, {"name": "api_monitored_contractids", "label": "API monitored contractids", "help_string": "Comma separated list of monitored contracts", "required": true, "format_type": "text", "default_value": "DC1,DC2", "placeholder": "", "type": "text", "value": "a"}, {"name": "use_splunk_helper_checkpoint", "label": "Use Splunk helper checkpoint", "help_string": "If possible leave checked to use Splunk helper checkpoint. If not it will fall back with files", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": false}, {"name": "events", "label": "Events", "help_string": "Log events", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": false}, {"name": "critical_events", "label": "Critical events", "help_string": "Log critical events", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}, {"name": "attack_reports", "label": "Attack reports", "help_string": "Log attack reports", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": false}, {"name": "api_minutes", "label": "Attack reports API minutes", "help_string": "Retrieve events in the last X minutes", "required": true, "format_type": "text", "default_value": "80640", "placeholder": "80640", "type": "text", "value": "80640"}], "data_inputs_options": [{"type": "customized_var", "name": "api_base_url", "title": "API base url", "description": "https://akab-yyyyyyyyyyyyyyyy-xxxxxxxxxxxxxxxx.luna.akamaiapis.net/", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_client_token", "title": "API client token", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_client_secret", "title": "API client secret", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "password", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_access_token", "title": "API access token", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_monitored_contractids", "title": "API monitored contractids", "description": "Comma separated list of monitored contracts", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "DC1,DC2", "placeholder": ""}, {"type": "customized_var", "name": "use_splunk_helper_checkpoint", "title": "Use Splunk helper checkpoint", "description": "If possible leave checked to use Splunk helper checkpoint. If not it will fall back with files", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "events", "title": "Events", "description": "Log events", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "critical_events", "title": "Critical events", "description": "Log critical events", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "attack_reports", "title": "Attack reports", "description": "Log attack reports", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "api_minutes", "title": "Attack reports API minutes", "description": "Retrieve events in the last X minutes", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "80640", "placeholder": "80640"}], "code": "\n# encoding = utf-8\n\nimport os\nimport sys\nimport time\nimport datetime\nimport requests\nimport hashlib\nimport json\nfrom akamai.edgegrid import EdgeGridAuth  # from https://github.com/akamai/AkamaiOPEN-edgegrid-python\n\n'''\n    IMPORTANT\n    Edit only the validate_input and collect_events functions.\n    Do not edit any other part in this file.\n    This file is generated only once when creating the modular input.\n'''\n'''\n# For advanced users, if you want to create single instance mod input, uncomment this method.\ndef use_single_instance_mode():\n    return True\n'''\n\ndef validate_input(helper, definition):\n    pass\n\n# based on the selected behaviour from use_helperCheckpoint this function will\n# use the helper get_check_point function (based on KV store) or fallback to\n# a JSON file to get the checkpoints for the attacks\ndef helper_getCheckpoint(helper, use_helperCheckpoint, key, filenameID):\n    if use_helperCheckpoint:\n        return helper.get_check_point(key)\n    else:\n        # get this python script path\n        scriptPath=os.path.dirname(os.path.realpath(__file__))\n        # ... create ...\n        filePath=scriptPath+ \"/events_\"+filenameID+\".json\"\n        fileExists=os.path.exists(filePath)\n        # ... load ...\n        data={}\n        if fileExists:\n            data = json.load(open(filePath))\n        else:\n            data={}\n        # ... and return the key value of the checkpoint\n        return data.get(key)\n        \n# based on the selected behaviour from use_helperCheckpoint this function will\n# use the helper save_check_point function (based on KV store) or fallback to\n# a JSON file to save the checkpoints for the attacks\ndef helper_setCheckpoint(helper, use_helperCheckpoint, key, value, filenameID):\n    if use_helperCheckpoint:\n        return helper.save_check_point(key, str(value))\n    else:\n        # get this python script path\n        scriptPath=os.path.dirname(os.path.realpath(__file__))\n        # ... create ...\n        filePath=scriptPath+ \"/events_\"+filenameID+\".json\"\n        fileExists=os.path.exists(filePath)\n        data={}\n        # ... load ...\n        if fileExists:\n            data = json.load(open(filePath))\n        else:\n            data={}\n        data[key]=value\n        # ... and save checkpoints on a JSON file\n        json.dump(data, open(filePath, 'w' ))\n\ndef collect_events(helper, ew):\n    \n    helper.log_info(\"Input {} has started.\".format(str(helper.get_input_stanza_names())))\n    \n    # setup for the proxy variables if needed\n    proxy = helper.get_proxy()\n    proxies = None\n    if proxy:\n        proxies = \\\n                {'https': '{}://{}:{}@{}:{}/'.format(proxy['proxy_type'\n                 ], proxy['proxy_username'], proxy['proxy_password'],\n                 proxy['proxy_url'], proxy['proxy_port']),\n                 'http': '{}://{}:{}@{}:{}/'.format(proxy['proxy_type'\n                 ], proxy['proxy_username'], proxy['proxy_password'],\n                 proxy['proxy_url'], proxy['proxy_port'])}\n    \n    helper.log_debug('start: collect_events')\n    \n    # get all the input variables configured\n    opt_api_base_url = helper.get_arg('api_base_url')\n    opt_api_client_token = helper.get_arg('api_client_token')\n    opt_api_client_secret = helper.get_arg('api_client_secret')\n    opt_api_access_token = helper.get_arg('api_access_token')\n    opt_api_monitored_contractids = helper.get_arg('api_monitored_contractids')\n    opt_use_splunk_helper_checkpoint = helper.get_arg('use_splunk_helper_checkpoint')\n    opt_critical_events = helper.get_arg('critical_events')\n    opt_events = helper.get_arg('events')\n    opt_attack_reports = helper.get_arg('attack_reports')\n    opt_api_minutes = helper.get_arg('api_minutes')\n    global_sslcertverification = helper.get_global_setting(\"sslcertverification\")\n    \n    # variable to save the current time and use it for the event time when logging\n    logTime = int(time.time())\n    \n    if opt_critical_events == True:\n        # run if the input is configured to collect the critical events from the API\n        helper.log_debug('akamai requests.Session() for critical events')\n        \n        # create a session object using the Akamai python helper\n        akamaiAPIsession = requests.Session()\n        helper.log_debug('akamai EdgeGridAuth() for critical events')\n        akamaiAPIsession.auth = EdgeGridAuth(opt_api_client_token,\n            opt_api_client_secret, opt_api_access_token)\n        \n        # for every contract perform an API call\n        contracts = opt_api_monitored_contractids.split(',')\n        for contract in contracts:\n            helper.log_debug('send GET critical events request')\n        \n            # run the GET request\n            result = akamaiAPIsession.get(opt_api_base_url+('prolexic-analytics/v2/critical-events/contract/'+contract), proxies=proxies,verify=global_sslcertverification)\n        \n            if(result.status_code==200):\n            \n                # now that we have a response is simply a matter of parsing the\n                # the returned JSON object to get the data that we want in a JSON\n                # format (one event for every line).\n                # Special attention is dedicated to avoid logging events already\n                # logged (with the help of checkpoints).\n                helper.log_debug('GET critical events response 200')\n                \n                # hash of the contract to better idenfity the checkpoint\n                contractHash = hashlib.md5((\"eventsCrit\"+str(contract)).encode('utf-8')).hexdigest()\n                responseJSON = result.json()\n            \n                if responseJSON[\"data\"] != None:\n                    for event in responseJSON[\"data\"]:\n                        # create an hash containing the eventID\n                        checkpointKey = hashlib.md5((\"critical-events\"+event[\"eventId\"]+contractHash).encode('utf-8')).hexdigest()\n                        # use it to retrieve the event hash\n                        eventCheckpointHash = helper_getCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,contractHash)\n                        if eventCheckpointHash == None:\n                            eventCheckpointHash = \"0\"\n                        \n                        # compute the event hash received from the API call\n                        eventHash = hashlib.md5(str(event).encode('utf-8')).hexdigest()\n                        if eventHash != eventCheckpointHash:\n                            # if the hash is different from the one retrieve from the checkpoint\n                            # log the new event...\n                            helper.log_debug('new critical event or update found, logging '+str(event[\"eventId\"]))\n                            \n                            # avoid fields with the name \"source\" in the logged data\n                            event[\"eventSource\"] = event[\"source\"]\n                            event.pop(\"source\", None)\n                            \n                            data = json.dumps(event)\n                            event = helper.new_event(time=logTime, source=helper.get_input_type(),\n                            index=helper.get_output_index(),\n                            sourcetype=helper.get_sourcetype(),\n                            data=data)\n                            ew.write_event(event)\n                            # ... and register the new hash so it wont be logged again next time\n                            helper_setCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,eventHash,contractHash)\n                        else:\n                            # nothing to do, the event hash was already registrered\n                            helper.log_debug('critical event hash found, skip logging')\n            else:\n                # dump the error in the logs\n                helper.log_error(result.text)\n    \n    if opt_events == True:\n        # run if the input is configured to collect events from the API\n        helper.log_debug('akamai requests.Session() events')\n        \n        # create a session object using the Akamai python helper\n        akamaiAPIsession = requests.Session()\n        helper.log_debug('akamai EdgeGridAuth() events')\n        akamaiAPIsession.auth = EdgeGridAuth(opt_api_client_token,\n            opt_api_client_secret, opt_api_access_token)\n\n        # for every contract perform an API call\n        contracts = opt_api_monitored_contractids.split(',')\n        for contract in contracts:\n            helper.log_debug('send GET events request')\n            \n            # run the GET request\n            result = akamaiAPIsession.get(opt_api_base_url+('prolexic-analytics/v2/events/contract/'+contract), proxies=proxies,verify=global_sslcertverification)\n        \n            if(result.status_code==200):\n            \n                # now that we have a response is simply a matter of parsing the\n                # the returned JSON object to get the data that we want in a JSON\n                # format (one event for every line).\n                # Special attention is dedicated to avoid logging events already\n                # logged (with the help of checkpoints).\n                helper.log_debug('GET events response 200')\n                \n                # hash of the contract to better idenfity the checkpoint\n                contractHash = hashlib.md5((\"events\"+str(contract)).encode('utf-8')).hexdigest()\n                responseJSON = result.json()\n            \n                if responseJSON[\"data\"] != None:\n                    for event in responseJSON[\"data\"]:\n                        eventId=\"\"\n                        if event[\"eventType\"] == \"alert\":\n                            eventId=\"attackId\"\n                        elif event[\"eventType\"] == \"attack\":\n                            eventId=\"attackEventId\"\n                        # use it to retrieve the event hash\n                        checkpointKey =hashlib.md5((\"events\"+event[\"eventInfo\"][eventId]+contractHash).encode('utf-8')).hexdigest()\n                        eventCheckpointHash = helper_getCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,contractHash)\n                        if eventCheckpointHash == None:\n                            eventCheckpointHash = \"0\"\n                        \n                        # compute the event hash received from the API call\n                        eventHash = hashlib.md5(str(event).encode('utf-8')).hexdigest()\n                        if eventHash != eventCheckpointHash:\n                            # if the hash is different from the one retrieve from the checkpoint\n                            # log the new event...\n                            helper.log_debug('hash not found, logging new event or update for '+contract+'->'+str(event[\"eventInfo\"][eventId]))\n                            data = json.dumps(event)\n                            event = helper.new_event(time=logTime, source=helper.get_input_type(),\n                                index=helper.get_output_index(),\n                                sourcetype=helper.get_sourcetype(),\n                                data=data)\n                            ew.write_event(event)\n                            # ... and register the new hash so it wont be logged again next time\n                            helper_setCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,eventHash,contractHash)\n                        else:\n                            # nothing to do, the event hash was already registrered\n                            helper.log_debug('event hash found, skip logging')\n                else:\n                    # dump the error in the logs\n                    helper.log_debug('responseJSON[data] is empty')\n            else:\n                helper.log_error(result.text)\n                \n    \n    if opt_attack_reports == True: \n        # run if the input is configured to collect attack reports from the API\n        helper.log_debug('akamai requests.Session() attack reports')\n        \n        # create a session object using the Akamai python helper\n        akamaiAPIsession = requests.Session()\n        helper.log_debug('akamai EdgeGridAuth() attack reports')\n        akamaiAPIsession.auth = EdgeGridAuth(opt_api_client_token,\n            opt_api_client_secret, opt_api_access_token)\n\n        # get the all the attack report from now going back opt_api_minutes minutes\n        epochTimeEnd= int(time.time())\n        epochTimeStart = epochTimeEnd - int(opt_api_minutes) * 60\n        \n        # for every contract perform an API call\n        contracts = opt_api_monitored_contractids.split(',')\n        for contract in contracts:\n            helper.log_debug('send GET attack reports request')\n            \n            # run the GET request\n            result = akamaiAPIsession.get(opt_api_base_url+('prolexic-analytics/v2/attack-reports/contract/'+contract+'/start/'+str(epochTimeStart)+'/end/'+str(epochTimeEnd)), proxies=proxies,verify=global_sslcertverification)\n        \n            if(result.status_code==200):\n            \n                helper.log_debug('POST attack reports response 200')\n            \n                contractHash = hashlib.md5((\"attacks\"+str(contract)).encode('utf-8')).hexdigest()\n                responseJSON = result.json()\n            \n                for event in responseJSON[\"data\"]:\n                    \n                    # now that we have a response is simply a matter of parsing the\n                    # the returned JSON object to get the data that we want in a JSON\n                    # format (one event for every line).\n                    # Special attention is dedicated to avoid logging events already\n                    # logged (with the help of checkpoints).\n                    checkpointKey=hashlib.md5(str('attack-reports-'+str(event[\"eventId\"])).encode('utf-8')).hexdigest()\n                    eventCheckpointHash = helper_getCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,contractHash)\n                    if eventCheckpointHash == None:\n                        eventCheckpointHash = \"0\"\n                    # hash of the contract to better idenfity the checkpoint\n                    eventHash=hashlib.md5((str(event)).encode('utf-8')).hexdigest()\n                    if eventHash != eventCheckpointHash:\n                        \n                        # if the hash is different from the one retrieve from the checkpoint\n                        # log the new event...\n                        helper.log_debug('new attack reports or update found, logging')\n                        data = json.dumps(event)\n                        event = helper.new_event(time=logTime, source=helper.get_input_type(),\n                            index=helper.get_output_index(),\n                            sourcetype=helper.get_sourcetype(),\n                            data=data)\n                        ew.write_event(event)\n                        # ... and register the new hash so it wont be logged again next time\n                        helper_setCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,eventHash,contractHash)\n                    else:\n                        # nothing to do, the event hash was already registrered\n                        helper.log_debug('attack reports hash found, skip logging')\n            else:\n                # dump the error in the logs\n                helper.log_error(result.text)\n    \n    helper.log_info(\"Input {} has ended.\".format(str(helper.get_input_stanza_names())))", "customized_options": [{"name": "api_base_url", "value": "a"}, {"name": "api_client_token", "value": "a"}, {"name": "api_access_token", "value": "a"}, {"name": "api_monitored_contractids", "value": "a"}, {"name": "use_splunk_helper_checkpoint", "value": false}, {"name": "events", "value": false}, {"name": "critical_events", "value": true}, {"name": "attack_reports", "value": false}, {"name": "api_minutes", "value": "80640"}], "uuid": "a7f47039ecef4d80b1984592d62120ae", "sample_count": 0}, {"index": "default", "sourcetype": "akamai:json_metrics", "interval": "60", "use_external_validation": true, "streaming_mode_xml": true, "name": "prolexic_metrics", "title": "Akamai metrics", "description": "Akamai Prolexic Analytics APIv2 metrics add on for Splunk", "type": "customized", "parameters": [{"name": "api_base_url", "label": "API base url", "help_string": "https://akab-yyyyyyyyyyyyyyyy-xxxxxxxxxxxxxxxx.luna.akamaiapis.net/", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "a"}, {"name": "api_client_token", "label": "API client token", "help_string": "", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "a"}, {"name": "api_client_secret", "label": "API client secret", "help_string": "", "required": true, "format_type": "password", "default_value": "", "placeholder": "", "type": "password", "value": ""}, {"name": "api_access_token", "label": "API access token", "help_string": "", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "a"}, {"name": "metrics", "label": "Metrics", "help_string": "Log metrics", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}, {"name": "api_monitored_contractids", "label": "API monitored contractids", "help_string": "Comma separated list of monitored contracts", "required": false, "format_type": "text", "default_value": "DC1,DC2", "placeholder": "", "type": "text", "value": "a"}, {"name": "time_series", "label": "Time series", "help_string": "Log time series", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}, {"name": "api_ip_assets", "label": "API IP assets", "help_string": "Comma separated list of monitored IP assets. One API call for each subnet and a final single API call for all remaining IPs. 1.1.1.0/24,1.1.2.0/24,1.1.1.1,1.1.3.1 -> 3 API calls.", "required": false, "format_type": "text", "default_value": "", "placeholder": "1.1.1.0/24,1.1.2.0/24,1.1.1.1,1.1.3.1", "type": "text", "value": "a"}, {"name": "api_sample", "label": "API sample", "help_string": "", "required": true, "format_type": "text", "default_value": "60", "placeholder": "30", "type": "text", "value": "10"}, {"name": "use_splunk_helper_checkpoint", "label": "Use Splunk helper checkpoint", "help_string": "If possible leave checked to use Splunk helper checkpoint. If not it will fall back with files", "required": false, "format_type": "checkbox", "default_value": true, "placeholder": "", "type": "checkbox", "value": false}], "data_inputs_options": [{"type": "customized_var", "name": "api_base_url", "title": "API base url", "description": "https://akab-yyyyyyyyyyyyyyyy-xxxxxxxxxxxxxxxx.luna.akamaiapis.net/", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_client_token", "title": "API client token", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_client_secret", "title": "API client secret", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "password", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_access_token", "title": "API access token", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "metrics", "title": "Metrics", "description": "Log metrics", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "api_monitored_contractids", "title": "API monitored contractids", "description": "Comma separated list of monitored contracts", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "DC1,DC2", "placeholder": ""}, {"type": "customized_var", "name": "time_series", "title": "Time series", "description": "Log time series", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}, {"type": "customized_var", "name": "api_ip_assets", "title": "API IP assets", "description": "Comma separated list of monitored IP assets. One API call for each subnet and a final single API call for all remaining IPs. 1.1.1.0/24,1.1.2.0/24,1.1.1.1,1.1.3.1 -> 3 API calls.", "required_on_edit": false, "required_on_create": false, "format_type": "text", "default_value": "", "placeholder": "1.1.1.0/24,1.1.2.0/24,1.1.1.1,1.1.3.1"}, {"type": "customized_var", "name": "api_sample", "title": "API sample", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "60", "placeholder": "30"}, {"type": "customized_var", "name": "use_splunk_helper_checkpoint", "title": "Use Splunk helper checkpoint", "description": "If possible leave checked to use Splunk helper checkpoint. If not it will fall back with files", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true, "placeholder": ""}], "code": " \n\n# encoding = utf-8\n\nimport os\nimport sys\nimport time\nimport datetime\nimport requests\nimport hashlib\nimport json\n# standard Akamai helper python script\nfrom akamai.edgegrid import EdgeGridAuth  # from https://github.com/akamai/AkamaiOPEN-edgegrid-python\n\ndef validate_input(helper, definition):\n    # just check it's an INT\n    api_sample = int(definition.parameters.get('api_sample', None))\n    # more checks can be done if needed\n    pass\n\n# based on the selected behaviour from use_helperCheckpoint this function will\n# use the helper get_check_point function (based on KV store) or fallback to\n# a JSON file to get the checkpoints\ndef helper_getCheckpoint(helper, use_helperCheckpoint, key, filenameID):\n    if use_helperCheckpoint:\n        return helper.get_check_point(key)\n    else:\n        # get this python script path\n        scriptPath=os.path.dirname(os.path.realpath(__file__))\n        # ... create ...\n        filePath=scriptPath+ \"/metrics_\"+filenameID+\".json\"\n        fileExists=os.path.exists(filePath)\n        # ... load ...\n        data={}\n        if fileExists:\n            data = json.load(open(filePath))\n        else:\n            data={}\n        # ... and return the key value of the checkpoint\n        return data.get(key)\n        \n# based on the selected behaviour from use_helperCheckpoint this function will\n# use the helper save_check_point function (based on KV store) or fallback to\n# a JSON file to save the checkpoints\ndef helper_setCheckpoint(helper, use_helperCheckpoint, key, value, filenameID):\n    if use_helperCheckpoint:\n        return helper.save_check_point(key, str(value))\n    else:\n        # get this python script path\n        scriptPath=os.path.dirname(os.path.realpath(__file__))\n        # ... create ...\n        filePath=scriptPath+ \"/metrics_\"+filenameID+\".json\"\n        fileExists=os.path.exists(filePath)\n        data={}\n        # ... load ...\n        if fileExists:\n            data = json.load(open(filePath))\n        else:\n            data={}\n        data[key]=value\n        # ... and save checkpoints on a JSON file\n        json.dump(data, open(filePath, 'w' ))\n\ndef collect_events(helper, ew):\n    \n    helper.log_info(\"Input {} has started.\".format(str(helper.get_input_stanza_names())))\n    \n    # setup for the proxy variables if needed\n    proxy = helper.get_proxy()\n    proxies = None\n    if proxy:\n        proxies = \\\n                {'https': '{}://{}:{}@{}:{}/'.format(proxy['proxy_type'\n                 ], proxy['proxy_username'], proxy['proxy_password'],\n                 proxy['proxy_url'], proxy['proxy_port']),\n                 'http': '{}://{}:{}@{}:{}/'.format(proxy['proxy_type'\n                 ], proxy['proxy_username'], proxy['proxy_password'],\n                 proxy['proxy_url'], proxy['proxy_port'])}\n\n    helper.log_debug('start: collect_events')\n\n    # get all the input variables configured\n    opt_api_base_url = helper.get_arg('api_base_url')\n    opt_api_client_token = helper.get_arg('api_client_token')\n    opt_api_client_secret = helper.get_arg('api_client_secret')\n    opt_api_access_token = helper.get_arg('api_access_token')\n    opt_metric = helper.get_arg('metrics')\n    opt_api_monitored_contractids = \\\n        helper.get_arg('api_monitored_contractids')\n    opt_api_sample = helper.get_arg('api_sample')\n    opt_use_splunk_helper_checkpoint = helper.get_arg('use_splunk_helper_checkpoint')\n    opt_time_series = helper.get_arg('time_series')\n    opt_api_ip_assets = helper.get_arg('api_ip_assets')\n    global_sslcertverification = helper.get_global_setting(\"sslcertverification\")\n\n    if opt_metric == True:\n        # run if the input is configured to collect the metrics from the API\n        helper.log_debug('akamai requests.Session() metrics')\n        \n        # create a session object using the Akamai python helper\n        akamaiAPIsession = requests.Session()\n        helper.log_debug('akamai EdgeGridAuth() metrics')\n        akamaiAPIsession.auth = EdgeGridAuth(opt_api_client_token,\n            opt_api_client_secret, opt_api_access_token)\n        helper.log_debug('create POST request JSON  metrics')\n\n        # prolexic-analytics/v2/metrics require a POST request with a JSON\n        # containing the time interval and a sample rate. \n        # The sample rate is fixed to one every minute\n        # The latest event is fixed to at most two minutes ago (rounded to the minute)\n        # The first sample is calculate by going back for the desidered amount\n        # of minutes configured in the input with the variable opt_api_sample\n        epochTimeEnd = int(time.time()) - 60\n        \n        # round to the previous minute\n        epochTimeEnd = epochTimeEnd - epochTimeEnd % 60\n        samplesMinutes = int(opt_api_sample)\n        \n        # go back samplesMinutes minutes\n        epochTimeStart = epochTimeEnd - 60 * (samplesMinutes) + 60\n\n        helper.log_debug('requesting  metrics data from ' + str(epochTimeStart)\n                     + ' until ' + str(epochTimeEnd) + ' sampled at ' + str(opt_api_sample))\n\n        # we need to do an API call for every single contract\n        contracts = opt_api_monitored_contractids.split(',')\n        for contract in contracts:\n        \n            # create the JSON for the POST request, other metrics are available\n            # but the ones below are the most stalbe\n            POSTdata = {\n                'contract': contract,\n                'start': epochTimeStart,\n                'end': epochTimeEnd,\n                'samples': samplesMinutes,\n                'type': {'mitigationPost': ['bandwidth', 'packets'],\n                         'mitigationPre': ['bandwidth', 'packets']},\n                }\n    \n            helper.log_debug('send POST request')\n        \n            # launch the POST request\n            result = akamaiAPIsession.post(opt_api_base_url\n                    + 'prolexic-analytics/v2/metrics', proxies=proxies,\n                    json=POSTdata, verify=global_sslcertverification)\n\n            if result.status_code == 200:\n            \n                helper.log_debug('POST  metrics response 200')\n            \n                # now that we have a response is simply a matter of parsing the\n                # the returned JSON object to get the data that we want in a JSON\n                # format in which every line represent a single time instance of\n                # a single metric. Special attention is dedicated to avoid logging\n                # events already logged (with the help of checkpoints)\n                responseJSON = result.json()\n                for metric in responseJSON['data']:\n                \n                    # sort the data by the epoch provided just in case\n                    lines = sorted(metric['points'], key=lambda k: int(k[0]), reverse=False)\n                \n                    # the hash of the contract is a good value to keep the checkpoint\n                    # files separated to avoid conflicts with multiple inputs\n                    contractHash = hashlib.md5((contract).encode('utf-8')).hexdigest()\n                    \n                    # a good hash to identify the specific metric which will be logged.\n                    # In this case [CONTRACT_ID][SERVICE][METRIC]\n                    checkpointKey = hashlib.md5(str(responseJSON['currentContract']+metric['service']+metric['metric']).encode('utf-8')).hexdigest()\n                    currentMetricCheckpoint = helper_getCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,contractHash)\n                    \n                    # if we can't find a checkpoint the code start with a default time of 0\n                    if currentMetricCheckpoint == None:\n                        currentMetricCheckpoint = '0'\n                    newMetricCheckpoint = currentMetricCheckpoint\n        \n                    for point in lines:\n                        # sometimes the returned value are invalid so we set them to -1\n                        if point[1] == None:\n                            point[1] = \"-1\"\n                        \n                        # create the JSON object\n                        data={}\n                        data[\"epochTimestamp\"]=int(point[0])\n                        data[\"DC\"]=responseJSON['currentContract']\n                        data[\"metric_name\"]=metric['service']\n                        data[\"metric_measure\"]=metric['metric']\n                        data[\"metric_value\"]=float(point[1])\n                    \n                        # create the event to be logged\n                        event = \\\n                            helper.new_event(time=float(point[0]),source=helper.get_input_type(),\n                                index=helper.get_output_index(),\n                                sourcetype=helper.get_sourcetype(),\n                                data=json.dumps(data))\n                    \n                        # now we check that the event that we want to be logged\n                        # is newer than what we already have logged (the checkpoint\n                        # is the last epoch logged for this specific metric in this contract)\n                        if int(point[0]) > int(currentMetricCheckpoint):\n                            helper.log_debug('new metric or update found, logging')\n                            # the event is newer so we log it and update the checkpoint\n                            ew.write_event(event)\n                            if int(point[0]) > int(newMetricCheckpoint):\n                                    newMetricCheckpoint=point[0]\n                    # now we have the checkpoint for the most recent epoch logged and\n                    # we must save it\n                    helper_setCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,str(newMetricCheckpoint),contractHash)\n            else:\n                # dump the error in the logs\n                helper.log_error(result.text)\n    \n    if opt_time_series == True:\n        # run if the input is configured to collect the time series data\n        helper.log_debug('akamai requests.Session() time series')\n        \n        # create a session object using the Akamai python helper\n        akamaiAPIsession = requests.Session()\n        helper.log_debug('akamai EdgeGridAuth() time series')\n        akamaiAPIsession.auth = EdgeGridAuth(opt_api_client_token,\n                opt_api_client_secret, opt_api_access_token)\n    \n        # prolexic-analytics/v2/time-series-data require a GET request containing\n        # the time interval and a sample rate. \n        # The sample rate is fixed to one every minute\n        # The latest event is fixed to at most two minutes ago (rounded to the minute)\n        # The first sample is calculate by going back for the desidered amount\n        # of minutes configured in the input with the variable opt_api_sample\n        epochTimeEnd = int(time.time()) - 60\n        epochTimeEnd = epochTimeEnd - epochTimeEnd % 60\n        samplesMinutes = int(opt_api_sample)\n        epochTimeStart = epochTimeEnd - 60 * (samplesMinutes) + 60\n\n        helper.log_debug('requesting time series data from ' + str(epochTimeStart)\n                     + ' until ' + str(epochTimeEnd) + ' sampled at ' + str(opt_api_sample))\n\n        # every subnet require an API call, but every IP can be done in single\n        # API call (if they are not too many). Start by getting all the IP and subnets\n        ipList=\"\"\n        subnets = opt_api_ip_assets.split(',')\n        helper.log_debug('starting logging subnets')\n        for subnet in subnets:\n        \n            if \"/\" in subnet:\n                #if we have a subnet we are good to go for an API call\n                helper.log_debug('send GET time series request for subnet '+subnet)\n        \n                # fill the fields correctly. locations=agr is to get the aggregate metrics (others are available)\n                result = akamaiAPIsession.get(opt_api_base_url+('prolexic-analytics/v2/time-series-data?destinations='+subnet+'&endTime='+str(epochTimeEnd)+'&locations=agr&samplingSize='+str(samplesMinutes)+'&startTime='+str(epochTimeStart)), proxies=proxies,verify=global_sslcertverification)\n\n                if result.status_code == 200:\n                    # now that we have a response is simply a matter of parsing the\n                    # the returned JSON object to get the data that we want in a JSON\n                    # format in which every line represent a single time instance of\n                    # a single metric. Special attention is dedicated to avoid logging\n                    # events already logged (with the help of checkpoints)\n                    helper.log_debug('GET time series response 200')\n                    responseJSON = result.json()\n                    for resource in responseJSON:\n                        for metric in responseJSON[resource]:\n                    \n                            # the hash of the subnet is a good value to keep the checkpoint\n                            # files separated to avoid conflicts with multiple inputs\n                            subnetHash = hashlib.md5((subnet).encode('utf-8')).hexdigest()\n                            \n                            # a good hash to identify the specific metrics which will be logged.\n                            # In this case IP-METRIC\n                            checkpointKey = hashlib.md5((subnet+\"-\"+metric).encode('utf-8')).hexdigest()\n                            \n                            currentMetricCheckpoint = helper_getCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,subnetHash)\n                            \n                            # if we can't find a checkpoint the code start with a default time of 0\n                            if currentMetricCheckpoint == None:\n                                currentMetricCheckpoint = '0'\n                            newMetricCheckpoint = currentMetricCheckpoint\n                            \n                            # sort the data by the epoch provided just in case\n                            lines = sorted(responseJSON[resource][metric], key=lambda k: int(k), reverse=False)\n                    \n                            for value in lines:\n                                # now we check that the event that we want to be logged\n                                # is newer than what we already have logged (the checkpoint\n                                # is the last epoch logged for this specific metric in this subnet)\n                                if int(value) > int(currentMetricCheckpoint):\n                                    helper.log_debug('new time series or update found, logging')\n                                \n                                    # create the JSON object\n                                    data={}\n                                    data[\"epochTimestamp\"]=int(str(value)[:-3])\n                                    data[\"asset\"]=resource\n                                    data[\"time_serie_name\"]=metric\n                                    data[\"time_serie_value\"]=float(responseJSON[resource][metric][value])\n                                \n                                    # create the event to be logged\n                                    event = helper.new_event(time=int(str(value)[:-3]),source=helper.get_input_type(),\n                                        index=helper.get_output_index(),\n                                        sourcetype=helper.get_sourcetype(),\n                                        data=json.dumps(data))\n                                    ew.write_event(event)\n                                    \n                                    # and save the new epoch if it's newer than what we have\n                                    if int(value) > int(newMetricCheckpoint):\n                                        newMetricCheckpoint=value\n                            # now we have the checkpoint for the most recent epoch logged and\n                            # we must save it\n                            helper_setCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,newMetricCheckpoint,subnetHash)\n                else:\n                    # dump the error in the logs\n                    helper.log_error(result.text)\n            else:\n                # if we don't have a subnet (so an IP) we fill an array with all of them\n                helper.log_debug('keeping '+subnet+ ' for later')\n                ipList=ipList+subnet+\",\"\n    \n        # just remove the last character (a comma)\n        ipList = ipList[:-1]\n    \n        # start again a single API call similar to what we did for the subnets\n        # but with a small formatting difference in the response\n        helper.log_debug('starting time series logging for IPs: '+ipList)\n\n        result = akamaiAPIsession.get(opt_api_base_url+\"prolexic-analytics/v2/time-series-data?destinations=\"+ipList+\"&endTime=\"+str(epochTimeEnd)+\"&locations=agr&samplingSize=\"+str(samplesMinutes)+\"&startTime=\"+str(epochTimeStart), proxies=proxies,verify=global_sslcertverification)\n\n        if result.status_code == 200:\n            \n            # now that we have a response is simply a matter of parsing the\n            #  the returned JSON object to get the data that we want in a JSON\n            # format in which every line represent a single time instance of\n            # a single metric. Special attention is dedicated to avoid logging\n            # events already logged (with the help of checkpoints)\n            helper.log_debug('GET time series response 200')\n            responseJSON = result.json()\n        \n            for asset in responseJSON:\n                for metric in responseJSON[asset]:\n                    \n                    # the hash of the subnets and IP is a good value to keep the checkpoint\n                    # files separated to avoid conflicts with multiple inputs\n                    assetHash = hashlib.md5((opt_api_ip_assets).encode('utf-8')).hexdigest()\n                    \n                    # a good hash to identify the specific metrics which will be logged.\n                    # In this case IP-METRIC\n                    checkpointKey = hashlib.md5((asset+\"-\"+metric).encode('utf-8')).hexdigest()\n                    currentMetricCheckpoint = helper_getCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,assetHash)\n\n                    # if we can't find a checkpoint the code start with a default time of 0\n                    if currentMetricCheckpoint == None:\n                        currentMetricCheckpoint = '0'\n                    newMetricCheckpoint = currentMetricCheckpoint\n                \n                    # sort the data by the epoch provided just in case\n                    lines = sorted(responseJSON[asset][metric], key=lambda k: int(k), reverse=False)\n                \n                    for value in lines:\n                        # now we check that the event that we want to be logged\n                        # is newer than what we already have logged (the checkpoint\n                        # is the last epoch logged for this specific metric in this IP list)\n                        if int(value) > int(currentMetricCheckpoint):\n                            helper.log_debug('new time series  or update found, logging')\n                        \n                            # create the JSON object\n                            data={}\n                            data[\"epochTimestamp\"]=int(str(value)[:-3])\n                            data[\"asset\"]=asset\n                            data[\"time_serie_name\"]=metric\n                            data[\"time_serie_value\"]=float(responseJSON[asset][metric][value])\n                            \n                            # create the event to be logged\n                            event = helper.new_event(time=int(str(value)[:-3]),source=helper.get_input_type(),\n                                index=helper.get_output_index(),\n                                sourcetype=helper.get_sourcetype(),\n                                data=json.dumps(data))\n                            ew.write_event(event)\n                            \n                            # and save the new epoch if it's newer than what we have\n                            if int(value) > int(newMetricCheckpoint):\n                                newMetricCheckpoint=value\n                     # now we have the checkpoint for the most recent epoch logged and\n                    # we must save it\n                    helper_setCheckpoint(helper, opt_use_splunk_helper_checkpoint,checkpointKey,newMetricCheckpoint,assetHash)\n        else:\n            # dump the error in the logs\n            helper.log_error(result.text)\n    \n    helper.log_info(\"Input {} has ended.\".format(str(helper.get_input_stanza_names())))", "customized_options": [{"name": "api_base_url", "value": "a"}, {"name": "api_client_token", "value": "a"}, {"name": "api_access_token", "value": "a"}, {"name": "metrics", "value": true}, {"name": "api_monitored_contractids", "value": "a"}, {"name": "time_series", "value": true}, {"name": "api_ip_assets", "value": "a"}, {"name": "api_sample", "value": "10"}, {"name": "use_splunk_helper_checkpoint", "value": false}], "uuid": "549e64fce5a848c38c308c03335d7a1b", "sample_count": 0}, {"index": "default", "sourcetype": "akamai:json_siem", "interval": "300", "use_external_validation": true, "streaming_mode_xml": true, "name": "akamai_siem", "title": "Akamai SIEM", "description": "Akamai SIEM API add on for Splunk", "type": "customized", "parameters": [{"name": "api_base_url", "label": "API base url", "help_string": "https://akab-yyyyyyyyyyyyyyyy-xxxxxxxxxxxxxxxx.luna.akamaiapis.net/", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "https://akab-xz4f5inhktr5vx4i-uwp3zm25psrgdmli.luna.akamaiapis.net/"}, {"name": "api_client_token", "label": "API client token", "help_string": "", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "akab-3yey3dpjowromear-7vrtljj526q3kmyw"}, {"name": "api_client_secret", "label": "API client secret", "help_string": "", "required": true, "format_type": "password", "default_value": "", "placeholder": "", "type": "password", "value": "3xzLZKviAPUSK4alXEBFdl+OopBKP+/cFeCH1kxH6sI="}, {"name": "api_access_token", "label": "API access token", "help_string": "", "required": true, "format_type": "text", "default_value": "", "placeholder": "", "type": "text", "value": "akab-pmovghtonr2nqoqn-hs7fhxkhquyrohph"}, {"name": "api_configid", "label": "API configID", "help_string": "Unique identifier for each security configuration. To report on more than one configuration, separate integer identifiers with semicolons.", "required": true, "format_type": "text", "default_value": "", "placeholder": "1111;2222", "type": "text", "value": "78268"}, {"name": "api_limit", "label": "API limit", "help_string": "Defines the approximate maximum number of security events each fetch returns.", "required": true, "format_type": "text", "default_value": "50000", "placeholder": "50000", "type": "text", "value": "40000"}, {"name": "time_limit", "label": "Time limit", "help_string": "Time limit in seconds after witch the log collection will stop", "required": true, "format_type": "text", "default_value": "240", "placeholder": "240", "type": "text", "value": "300"}, {"name": "use_splunk_helper_checkpoint", "label": "Use Splunk helper checkpoint", "help_string": "If possible leave checked to use Splunk helper checkpoint. If not it will fall back with files", "required": false, "format_type": "checkbox", "default_value": true, "type": "checkbox", "value": true}], "data_inputs_options": [{"type": "customized_var", "name": "api_base_url", "title": "API base url", "description": "https://akab-yyyyyyyyyyyyyyyy-xxxxxxxxxxxxxxxx.luna.akamaiapis.net/", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_client_token", "title": "API client token", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_client_secret", "title": "API client secret", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "password", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_access_token", "title": "API access token", "description": "", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": ""}, {"type": "customized_var", "name": "api_configid", "title": "API configID", "description": "Unique identifier for each security configuration. To report on more than one configuration, separate integer identifiers with semicolons.", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "", "placeholder": "1111;2222"}, {"type": "customized_var", "name": "api_limit", "title": "API limit", "description": "Defines the approximate maximum number of security events each fetch returns.", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "50000", "placeholder": "50000"}, {"type": "customized_var", "name": "time_limit", "title": "Time limit", "description": "Time limit in seconds after witch the log collection will stop", "required_on_edit": false, "required_on_create": true, "format_type": "text", "default_value": "240", "placeholder": "240"}, {"type": "customized_var", "name": "use_splunk_helper_checkpoint", "title": "Use Splunk helper checkpoint", "description": "If possible leave checked to use Splunk helper checkpoint. If not it will fall back with files", "required_on_edit": false, "required_on_create": false, "format_type": "checkbox", "default_value": true}], "code": "\n# encoding = utf-8\n\nimport os\nimport sys\nimport time\nimport datetime\nimport requests\nimport sys\nimport json\nimport time\nimport re\nimport hashlib\nimport base64\nimport traceback\nfrom akamai.edgegrid import EdgeGridAuth  # from https://github.com/akamai/AkamaiOPEN-edgegrid-python\nfrom urllib.parse import unquote\n\n'''\n    IMPORTANT\n    Edit only the validate_input and collect_events functions.\n    Do not edit any other part in this file.\n    This file is generated only once when creating the modular input.\n'''\n'''\n# For advanced users, if you want to create single instance mod input, uncomment this method.\ndef use_single_instance_mode():\n    return True\n'''\n\ndef validate_input(helper, definition):\n    \"\"\"Implement your own validation logic to validate the input stanza configurations\"\"\"\n    # This example accesses the modular input variable\n    # api_base_url = definition.parameters.get('api_base_url', None)\n    # api_client_token = definition.parameters.get('api_client_token', None)\n    # api_client_secret = definition.parameters.get('api_client_secret', None)\n    # api_access_token = definition.parameters.get('api_access_token', None)\n    # api_configid = definition.parameters.get('api_configid', None)\n    # api_limit = definition.parameters.get('api_limit', None)\n    # time_limit = definition.parameters.get('time_limit', None)\n    # use_splunk_helper_checkpoint = definition.parameters.get('use_splunk_helper_checkpoint', None)\n    pass\n\n# based on the selected behaviour from use_helperCheckpoint this function will\n# use the helper get_check_point function (based on KV store) or fallback to\n# a JSON file to get the checkpoints for the attacks\ndef helper_getCheckpoint(helper, use_helperCheckpoint, key, filenameID):\n    if use_helperCheckpoint:\n        return helper.get_check_point(key)\n    else:\n        # get this python script path\n        scriptPath=os.path.dirname(os.path.realpath(__file__))\n        # ... create ...\n        filePath=scriptPath+ \"/siem_\"+filenameID+\".json\"\n        fileExists=os.path.exists(filePath)\n        # ... load ...\n        data={}\n        if fileExists:\n            data = json.load(open(filePath))\n        else:\n            data={}\n        # ... and return the key value of the checkpoint\n        return data.get(key)\n        \n# based on the selected behaviour from use_helperCheckpoint this function will\n# use the helper save_check_point function (based on KV store) or fallback to\n# a JSON file to save the checkpoints for the attacks\ndef helper_setCheckpoint(helper, use_helperCheckpoint, key, value, filenameID):\n    if use_helperCheckpoint:\n        return helper.save_check_point(key, str(value))\n    else:\n        # get this python script path\n        scriptPath=os.path.dirname(os.path.realpath(__file__))\n        # ... create ...\n        filePath=scriptPath+ \"/siem_\"+filenameID+\".json\"\n        fileExists=os.path.exists(filePath)\n        data={}\n        # ... load ...\n        if fileExists:\n            data = json.load(open(filePath))\n        else:\n            data={}\n        data[key]=value\n        # ... and save checkpoints on a JSON file\n        json.dump(data, open(filePath, 'w' ))\n\ndef collect_events(helper, ew):\n    \n    helper.log_info(\"Input {} has started.\".format(str(helper.get_input_stanza_names())))\n    \n    # setup for the proxy variables if needed\n    proxy = helper.get_proxy()\n    proxies = None\n    if proxy:\n        proxies = \\\n                {'https': '{}://{}:{}@{}:{}/'.format(proxy['proxy_type'\n                 ], proxy['proxy_username'], proxy['proxy_password'],\n                 proxy['proxy_url'], proxy['proxy_port']),\n                 'http': '{}://{}:{}@{}:{}/'.format(proxy['proxy_type'\n                 ], proxy['proxy_username'], proxy['proxy_password'],\n                 proxy['proxy_url'], proxy['proxy_port'])}\n                 \n    opt_api_base_url = helper.get_arg('api_base_url')\n    opt_api_client_token = helper.get_arg('api_client_token')\n    opt_api_client_secret = helper.get_arg('api_client_secret')\n    opt_api_access_token = helper.get_arg('api_access_token')\n    opt_api_configid = helper.get_arg('api_configid')\n    opt_api_limit = int(helper.get_arg('api_limit'))\n    opt_time_limit = int(helper.get_arg('time_limit'))\n    opt_use_splunk_helper_checkpoint = helper.get_arg('use_splunk_helper_checkpoint')\n    global_sslcertverification = helper.get_global_setting(\"sslcertverification\")\n    \n    # create a session object using the Akamai python helper\n    akamaiAPIsession = requests.Session()\n    helper.log_debug('Akamai EdgeGridAuth() for SIEM events')\n    akamaiAPIsession.auth = EdgeGridAuth(opt_api_client_token, opt_api_client_secret, opt_api_access_token)\n    \n    start_time = int(time.time())\n    \n    configIDhash = hashlib.md5((str(opt_api_configid)).encode('utf-8')).hexdigest()\n    \n    eventCheckpoint = helper_getCheckpoint(helper, opt_use_splunk_helper_checkpoint,configIDhash,configIDhash)\n    if eventCheckpoint == None:\n        helper.log_debug('Checkpoint not found')\n        eventCheckpoint = \"NULL\"\n    \n    helper.log_debug('Checkpoint is '+eventCheckpoint)\n    \n    while int(time.time()) - start_time < opt_time_limit:\n        \n        helper.log_debug('Send GET SIEM request')\n        \n        result = akamaiAPIsession.get(opt_api_base_url+'siem/v1/configs/'+str(opt_api_configid)+'?offset='+eventCheckpoint+'&limit='+str(int(opt_api_limit)), proxies=proxies,verify=global_sslcertverification)\n        \n        if result.status_code==200:\n            \n            helper.log_debug('GET SIEM response 200')\n            helper.log_debug('Parsing event JSON data')\n            \n            # split every line is a single object...\n            JSONobjects=result.text.splitlines()\n            \n            # ... and iterate over them\n            for event in JSONobjects:\n                # valid attack event\n                if \"attackData\" in event:\n                    \n                    try:\n                        eventJSON=json.loads(event)\n                        rules_array = []\n                        other_data = {}\n                        for member in eventJSON[\"attackData\"]:\n                            # if the field start with \"rule\" we need a special parsing\n                            # code adapted from the official API docs\n                            if member[0:4] == 'rule':\n                                # Alternate field name converted from plural:\n                                member_as_singular = re.sub(\"s$\", \"\", member)\n                                url_decoded = unquote(eventJSON[\"attackData\"][member])\n                                # remove empty strings\n                                member_array = list(filter(None, url_decoded.split(\";\")))\n                                if not len(rules_array):\n                                    for i in range(len(member_array)):\n                                        rules_array.append({})\n                                i = 0\n                                for item in member_array:\n                                    rules_array[i][member_as_singular] = base64.b64decode(item).decode(\"UTF-8\",errors='replace')\n                                    i += 1\n                            # if doesn't start with \"rule\" is data we need to keep for later\n                            else:\n                                other_data[member]=eventJSON[\"attackData\"][member]\n                        # replace the rules data with the parsed format...\n                        eventJSON[\"attackData\"]=rules_array\n                        # ... and add other data\n                        eventJSON[\"attackData\"].append(other_data)\n                        \n                        # remove some fields\n                        eventJSON.pop(\"format\", None)\n                        eventJSON.pop(\"type\", None)\n                        eventJSON.pop(\"version\", None) \n                    \n                        # log the data with the time taken from the JSON event\n                        data = json.dumps(eventJSON)\n                        event = helper.new_event(time=int(eventJSON[\"httpMessage\"][\"start\"]), source=helper.get_input_type(),\n                            index=helper.get_output_index(),\n                            sourcetype=helper.get_sourcetype(),\n                            data=data)\n                        ew.write_event(event)\n                    except ValueError:  # includes simplejson.decoder.JSONDecodeError\n                        helper.log_debug('Decoding JSON has failed')\n                        helper.log_debug(traceback.format_exc())\n                        helper.log_debug(\"Resetting offset because of an error, setting offset=NULL to restart\")\n                        eventCheckpoint=\"NULL\"\n                        helper_setCheckpoint(helper, opt_use_splunk_helper_checkpoint,configIDhash,eventCheckpoint,configIDhash)\n                # if the obejct contain an offset, save it\n                elif \"offset\" in event:\n                    eventJSON=json.loads(event)\n                    eventCheckpoint=eventJSON[\"offset\"]\n                    helper.log_debug(\"Fetched \" + str(eventJSON[\"total\"]) + \" events with final offset \" + eventJSON[\"offset\"])\n                    helper_setCheckpoint(helper, opt_use_splunk_helper_checkpoint,configIDhash,eventCheckpoint,configIDhash)\n                    \n                    # if we collected 0 events end the script\n                    if eventJSON[\"total\"] == 0:\n                        helper.log_debug(\"Ending the collection script\")\n                        opt_time_limit=0\n                # shoul never go here, log in case it happens\n                else:\n                    helper.log_debug(\"Unknown data \"+event)\n        elif result.status_code==416:\n            # dump the error in the logs\n            helper.log_error(result.text)\n            # in case of old offset (result code=416) set offset=NULL for a fresh restart\n            helper.log_debug(\"Old offset, setting offset=NULL to restart\")\n            eventCheckpoint=\"NULL\"\n            helper_setCheckpoint(helper, opt_use_splunk_helper_checkpoint,configIDhash,eventCheckpoint,configIDhash)\n            \n        else:\n            # dump the error in the logs\n            helper.log_error(result.text)\n            # exit the loop\n            opt_time_limit=0\n    \n    helper.log_info(\"Input {} has ended.\".format(str(helper.get_input_stanza_names())))", "customized_options": [{"name": "api_base_url", "value": "aa"}, {"name": "api_client_token", "value": "a"}, {"name": "api_access_token", "value": "a"}, {"name": "api_configid", "value": "a"}, {"name": "api_limit", "value": "a"}, {"name": "time_limit", "value": "a"}, {"name": "use_splunk_helper_checkpoint", "value": true}], "uuid": "448e219052934aa5964958b74945dc22", "sample_count": 0}]}, "field_extraction_builder": {"akamai:json_metrics": {"is_parsed": true, "data_format": "json"}, "akamai:json_conf": {"is_parsed": true, "data_format": "json"}, "akamai:json_event": {"is_parsed": true, "data_format": "json"}, "akamai:json_siem": {"is_parsed": true, "data_format": "json"}}, "global_settings_builder": {"global_settings": {"proxy_settings": {"proxy_type": "http"}, "log_settings": {"log_level": "DEBUG"}, "credential_settings": [], "customized_settings": [{"required": false, "name": "sslcertverification", "label": "SSLCertVerification", "default_value": true, "help_string": "Verify the SSL certificate for HTTPS requests", "type": "checkbox", "format_type": "checkbox", "value": true}]}}, "sourcetype_builder": {"akamai:json_conf": {"metadata": {"event_count": 0, "data_input_name": "conf_domains", "extractions_count": 0, "cims_count": 0}}, "akamai:json_event": {"metadata": {"event_count": 0, "data_input_name": "prolexic_events", "extractions_count": 0, "cims_count": 0}}, "akamai:json_metrics": {"metadata": {"event_count": 0, "data_input_name": "prolexic_metrics", "extractions_count": 0, "cims_count": 0}}, "akamai:json_siem": {"metadata": {"event_count": 0, "data_input_name": "akamai_siem", "extractions_count": 0, "cims_count": 0}}}, "validation": {"validators": ["best_practice_validation", "data_model_mapping_validation", "field_extract_validation", "app_cert_validation"], "status": "job_started", "validation_id": "v_1647074604_96"}}